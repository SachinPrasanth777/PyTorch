{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNf8UER67EE3yZO8SR5MJbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SachinPrasanth777/PyTorch/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "60Mc4SlyjeHd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('anna.txt','r') as f:\n",
        "  text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPfq00rVkhNW",
        "outputId": "281b45c1-8795-45ae-f099-190a1f3d747c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii,ch in int2char.items()}\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "print(encoded[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nISO7VnlH5r",
        "outputId": "cfe3d7e4-f44d-41e3-bb90-0198e9705b31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42 22 13  0 60 64 76 32 38 73 73 73  5 13  0  0 65 32 14 13 78 15 20 15\n",
            " 64 36 32 13 76 64 32 13 20 20 32 13 20 15 50 64 58 32 64 75 64 76 65 32\n",
            "  8 11 22 13  0  0 65 32 14 13 78 15 20 65 32 15 36 32  8 11 22 13  0  0\n",
            " 65 32 15 11 32 15 60 36 32 70 21 11 73 21 13 65 23 73 73 46 75 64 76 65\n",
            " 60 22 15 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr,n_labels):\n",
        "  one_hot = np.zeros((arr.size,n_labels), dtype = np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "  one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "J3cgvrJZnRy-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9sy0EKloYL3",
        "outputId": "fb364c79-0206-4b2f-d0ec-fd3421148615"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr,batch_size,seq_length):\n",
        "  batch_size_total = batch_size * seq_length\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "  arr = arr[:n_batches * batch_size_total]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "  x = np.zeros((batch_size, seq_length), dtype=arr.dtype)\n",
        "  y = np.zeros_like(x)\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "    x = arr[:, n:n+seq_length]\n",
        "    y = np.zeros_like(x)\n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x,y"
      ],
      "metadata": {
        "id": "jaqFCz8rpyje"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded,8,50)\n",
        "x, y = next(batches)"
      ],
      "metadata": {
        "id": "2_J9sqQir4e0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OExYMjrseoR",
        "outputId": "8c804fe7-8fca-4007-b7d9-27ce57e4ddd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[42 22 13  0 60 64 76 32 38 73]\n",
            " [36 70 11 32 60 22 13 60 32 13]\n",
            " [64 11 55 32 70 76 32 13 32 14]\n",
            " [36 32 60 22 64 32 82 22 15 64]\n",
            " [32 36 13 21 32 22 64 76 32 60]\n",
            " [82  8 36 36 15 70 11 32 13 11]\n",
            " [32  2 11 11 13 32 22 13 55 32]\n",
            " [63 24 20 70 11 36 50 65 23 32]]\n",
            "\n",
            "y\n",
            " [[22 13  0 60 64 76 32 38 73 73]\n",
            " [70 11 32 60 22 13 60 32 13 60]\n",
            " [11 55 32 70 76 32 13 32 14 70]\n",
            " [32 60 22 64 32 82 22 15 64 14]\n",
            " [36 13 21 32 22 64 76 32 60 64]\n",
            " [ 8 36 36 15 70 11 32 13 11 55]\n",
            " [ 2 11 11 13 32 22 13 55 32 36]\n",
            " [24 20 70 11 36 50 65 23 32 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5IBVez6taHw",
        "outputId": "e60f1a67-c031-42b9-8d73-d1423c0e5c95"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,tokens,n_hidden=256,n_layers=2,drop_prob=0.5,lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "    self.chars = tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        out = self.dropout(r_output)\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "  def init_hidden(self,batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers,batch_size, self.n_hidden).zero_())\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "HBDLLl4MuY_w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net,data,epochs=10,batch_size=10,seq_length=50,lr=0.001,clip=5,val_frac=0.1,print_every=10):\n",
        "  net.train()\n",
        "  opt = torch.optim.Adam(net.parameters(),lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  val_idx = int(len(data)*(1-val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "    for x,y in get_batches(data,batch_size,seq_length):\n",
        "      counter += 1\n",
        "      x = one_hot_encode(x,n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      if train_on_gpu():\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      h = tuple([each.data for each in h])\n",
        "      net.zero_grad()\n",
        "      output, h = net(inputs,h)\n",
        "      loss = criterion(output,targets.view(batch_size*seq_length).long())\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "      opt.step()\n",
        "      if counter % print_every == 0:\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                    val_losses.append(val_loss.item())\n",
        "                net.train()\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "ChYcRz17YisY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "net = RNN(chars,n_hidden=512,n_layers=2)\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlYQZ5HTixjp",
        "outputId": "ad2866d3-b7dd-45c3-8942-3e5cc6154f2b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2478... Val Loss: 3.1738\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1514... Val Loss: 3.1285\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1411... Val Loss: 3.1212\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1186... Val Loss: 3.1195\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1445... Val Loss: 3.1180\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1190... Val Loss: 3.1165\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1077... Val Loss: 3.1158\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1247... Val Loss: 3.1130\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1222... Val Loss: 3.1075\n",
            "Epoch: 1/20... Step: 100... Loss: 3.1060... Val Loss: 3.0983\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0921... Val Loss: 3.0741\n",
            "Epoch: 1/20... Step: 120... Loss: 3.0269... Val Loss: 3.0361\n",
            "Epoch: 1/20... Step: 130... Loss: 2.9832... Val Loss: 2.9600\n",
            "Epoch: 2/20... Step: 140... Loss: 2.9185... Val Loss: 2.8645\n",
            "Epoch: 2/20... Step: 150... Loss: 2.8347... Val Loss: 2.7786\n",
            "Epoch: 2/20... Step: 160... Loss: 2.7384... Val Loss: 2.6907\n",
            "Epoch: 2/20... Step: 170... Loss: 2.6285... Val Loss: 2.6194\n",
            "Epoch: 2/20... Step: 180... Loss: 2.5921... Val Loss: 2.5650\n",
            "Epoch: 2/20... Step: 190... Loss: 2.5488... Val Loss: 2.5258\n",
            "Epoch: 2/20... Step: 200... Loss: 2.5465... Val Loss: 2.5059\n",
            "Epoch: 2/20... Step: 210... Loss: 2.4997... Val Loss: 2.4689\n",
            "Epoch: 2/20... Step: 220... Loss: 2.4577... Val Loss: 2.4360\n",
            "Epoch: 2/20... Step: 230... Loss: 2.4419... Val Loss: 2.4120\n",
            "Epoch: 2/20... Step: 240... Loss: 2.4388... Val Loss: 2.3904\n",
            "Epoch: 2/20... Step: 250... Loss: 2.3782... Val Loss: 2.3658\n",
            "Epoch: 2/20... Step: 260... Loss: 2.3542... Val Loss: 2.3407\n",
            "Epoch: 2/20... Step: 270... Loss: 2.3503... Val Loss: 2.3160\n",
            "Epoch: 3/20... Step: 280... Loss: 2.3579... Val Loss: 2.2982\n",
            "Epoch: 3/20... Step: 290... Loss: 2.3296... Val Loss: 2.2775\n",
            "Epoch: 3/20... Step: 300... Loss: 2.2947... Val Loss: 2.2633\n",
            "Epoch: 3/20... Step: 310... Loss: 2.2863... Val Loss: 2.2353\n",
            "Epoch: 3/20... Step: 320... Loss: 2.2528... Val Loss: 2.2158\n",
            "Epoch: 3/20... Step: 330... Loss: 2.2201... Val Loss: 2.1971\n",
            "Epoch: 3/20... Step: 340... Loss: 2.2310... Val Loss: 2.1844\n",
            "Epoch: 3/20... Step: 350... Loss: 2.2201... Val Loss: 2.1663\n",
            "Epoch: 3/20... Step: 360... Loss: 2.1565... Val Loss: 2.1477\n",
            "Epoch: 3/20... Step: 370... Loss: 2.1775... Val Loss: 2.1346\n",
            "Epoch: 3/20... Step: 380... Loss: 2.1600... Val Loss: 2.1203\n",
            "Epoch: 3/20... Step: 390... Loss: 2.1312... Val Loss: 2.1017\n",
            "Epoch: 3/20... Step: 400... Loss: 2.1058... Val Loss: 2.0839\n",
            "Epoch: 3/20... Step: 410... Loss: 2.1255... Val Loss: 2.0742\n",
            "Epoch: 4/20... Step: 420... Loss: 2.0930... Val Loss: 2.0567\n",
            "Epoch: 4/20... Step: 430... Loss: 2.0905... Val Loss: 2.0450\n",
            "Epoch: 4/20... Step: 440... Loss: 2.0789... Val Loss: 2.0328\n",
            "Epoch: 4/20... Step: 450... Loss: 2.0087... Val Loss: 2.0148\n",
            "Epoch: 4/20... Step: 460... Loss: 2.0125... Val Loss: 2.0026\n",
            "Epoch: 4/20... Step: 470... Loss: 2.0360... Val Loss: 1.9882\n",
            "Epoch: 4/20... Step: 480... Loss: 2.0051... Val Loss: 1.9888\n",
            "Epoch: 4/20... Step: 490... Loss: 2.0176... Val Loss: 1.9660\n",
            "Epoch: 4/20... Step: 500... Loss: 2.0062... Val Loss: 1.9549\n",
            "Epoch: 4/20... Step: 510... Loss: 1.9881... Val Loss: 1.9401\n",
            "Epoch: 4/20... Step: 520... Loss: 2.0007... Val Loss: 1.9275\n",
            "Epoch: 4/20... Step: 530... Loss: 1.9508... Val Loss: 1.9223\n",
            "Epoch: 4/20... Step: 540... Loss: 1.9069... Val Loss: 1.9049\n",
            "Epoch: 4/20... Step: 550... Loss: 1.9512... Val Loss: 1.8926\n",
            "Epoch: 5/20... Step: 560... Loss: 1.9296... Val Loss: 1.8854\n",
            "Epoch: 5/20... Step: 570... Loss: 1.9074... Val Loss: 1.8761\n",
            "Epoch: 5/20... Step: 580... Loss: 1.8835... Val Loss: 1.8617\n",
            "Epoch: 5/20... Step: 590... Loss: 1.8901... Val Loss: 1.8545\n",
            "Epoch: 5/20... Step: 600... Loss: 1.8685... Val Loss: 1.8449\n",
            "Epoch: 5/20... Step: 610... Loss: 1.8554... Val Loss: 1.8382\n",
            "Epoch: 5/20... Step: 620... Loss: 1.8557... Val Loss: 1.8282\n",
            "Epoch: 5/20... Step: 630... Loss: 1.8763... Val Loss: 1.8156\n",
            "Epoch: 5/20... Step: 640... Loss: 1.8467... Val Loss: 1.8053\n",
            "Epoch: 5/20... Step: 650... Loss: 1.8236... Val Loss: 1.7973\n",
            "Epoch: 5/20... Step: 660... Loss: 1.8007... Val Loss: 1.7887\n",
            "Epoch: 5/20... Step: 670... Loss: 1.8192... Val Loss: 1.7821\n",
            "Epoch: 5/20... Step: 680... Loss: 1.8160... Val Loss: 1.7725\n",
            "Epoch: 5/20... Step: 690... Loss: 1.7931... Val Loss: 1.7703\n",
            "Epoch: 6/20... Step: 700... Loss: 1.7779... Val Loss: 1.7559\n",
            "Epoch: 6/20... Step: 710... Loss: 1.7829... Val Loss: 1.7489\n",
            "Epoch: 6/20... Step: 720... Loss: 1.7640... Val Loss: 1.7417\n",
            "Epoch: 6/20... Step: 730... Loss: 1.7778... Val Loss: 1.7336\n",
            "Epoch: 6/20... Step: 740... Loss: 1.7514... Val Loss: 1.7273\n",
            "Epoch: 6/20... Step: 750... Loss: 1.7204... Val Loss: 1.7226\n",
            "Epoch: 6/20... Step: 760... Loss: 1.7600... Val Loss: 1.7131\n",
            "Epoch: 6/20... Step: 770... Loss: 1.7392... Val Loss: 1.7083\n",
            "Epoch: 6/20... Step: 780... Loss: 1.7162... Val Loss: 1.7003\n",
            "Epoch: 6/20... Step: 790... Loss: 1.7219... Val Loss: 1.6948\n",
            "Epoch: 6/20... Step: 800... Loss: 1.7235... Val Loss: 1.6904\n",
            "Epoch: 6/20... Step: 810... Loss: 1.7127... Val Loss: 1.6839\n",
            "Epoch: 6/20... Step: 820... Loss: 1.6911... Val Loss: 1.6734\n",
            "Epoch: 6/20... Step: 830... Loss: 1.7162... Val Loss: 1.6707\n",
            "Epoch: 7/20... Step: 840... Loss: 1.6695... Val Loss: 1.6644\n",
            "Epoch: 7/20... Step: 850... Loss: 1.6876... Val Loss: 1.6595\n",
            "Epoch: 7/20... Step: 860... Loss: 1.6730... Val Loss: 1.6554\n",
            "Epoch: 7/20... Step: 870... Loss: 1.6868... Val Loss: 1.6462\n",
            "Epoch: 7/20... Step: 880... Loss: 1.6779... Val Loss: 1.6424\n",
            "Epoch: 7/20... Step: 890... Loss: 1.6681... Val Loss: 1.6382\n",
            "Epoch: 7/20... Step: 900... Loss: 1.6507... Val Loss: 1.6339\n",
            "Epoch: 7/20... Step: 910... Loss: 1.6305... Val Loss: 1.6289\n",
            "Epoch: 7/20... Step: 920... Loss: 1.6522... Val Loss: 1.6218\n",
            "Epoch: 7/20... Step: 930... Loss: 1.6270... Val Loss: 1.6176\n",
            "Epoch: 7/20... Step: 940... Loss: 1.6347... Val Loss: 1.6124\n",
            "Epoch: 7/20... Step: 950... Loss: 1.6378... Val Loss: 1.6070\n",
            "Epoch: 7/20... Step: 960... Loss: 1.6398... Val Loss: 1.6034\n",
            "Epoch: 7/20... Step: 970... Loss: 1.6462... Val Loss: 1.6042\n",
            "Epoch: 8/20... Step: 980... Loss: 1.6244... Val Loss: 1.5997\n",
            "Epoch: 8/20... Step: 990... Loss: 1.6265... Val Loss: 1.5957\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.6108... Val Loss: 1.5870\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.6489... Val Loss: 1.5817\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.6074... Val Loss: 1.5800\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.5974... Val Loss: 1.5748\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.6046... Val Loss: 1.5726\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.5906... Val Loss: 1.5690\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.5940... Val Loss: 1.5611\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5965... Val Loss: 1.5589\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5883... Val Loss: 1.5551\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.5760... Val Loss: 1.5553\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.5613... Val Loss: 1.5491\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.5712... Val Loss: 1.5451\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.5814... Val Loss: 1.5471\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.5713... Val Loss: 1.5378\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.5734... Val Loss: 1.5333\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5811... Val Loss: 1.5332\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.5398... Val Loss: 1.5310\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.5449... Val Loss: 1.5300\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.5491... Val Loss: 1.5248\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.5697... Val Loss: 1.5192\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.5137... Val Loss: 1.5161\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.5345... Val Loss: 1.5125\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.5261... Val Loss: 1.5138\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.5087... Val Loss: 1.5082\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.5086... Val Loss: 1.5049\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.5246... Val Loss: 1.5022\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.5380... Val Loss: 1.5026\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.5228... Val Loss: 1.4962\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.5336... Val Loss: 1.4934\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.5212... Val Loss: 1.4906\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.5061... Val Loss: 1.4910\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.5250... Val Loss: 1.4903\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4926... Val Loss: 1.4878\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4958... Val Loss: 1.4811\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4841... Val Loss: 1.4758\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.4604... Val Loss: 1.4758\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4765... Val Loss: 1.4739\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.4655... Val Loss: 1.4746\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4961... Val Loss: 1.4698\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.5014... Val Loss: 1.4662\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.5016... Val Loss: 1.4641\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.5155... Val Loss: 1.4610\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4966... Val Loss: 1.4590\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4663... Val Loss: 1.4567\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.5042... Val Loss: 1.4581\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.4333... Val Loss: 1.4580\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.4487... Val Loss: 1.4555\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.4511... Val Loss: 1.4511\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.4670... Val Loss: 1.4480\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.4551... Val Loss: 1.4477\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.4354... Val Loss: 1.4483\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.4273... Val Loss: 1.4454\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.4637... Val Loss: 1.4416\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.5052... Val Loss: 1.4390\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.4583... Val Loss: 1.4370\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.4683... Val Loss: 1.4341\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4707... Val Loss: 1.4329\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.4257... Val Loss: 1.4323\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3951... Val Loss: 1.4295\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3986... Val Loss: 1.4297\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.4287... Val Loss: 1.4297\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.4115... Val Loss: 1.4265\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.4231... Val Loss: 1.4204\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.4385... Val Loss: 1.4232\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.4109... Val Loss: 1.4240\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3906... Val Loss: 1.4182\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.4460... Val Loss: 1.4172\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.4180... Val Loss: 1.4148\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.4273... Val Loss: 1.4137\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.4017... Val Loss: 1.4092\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.4074... Val Loss: 1.4095\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3819... Val Loss: 1.4082\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3907... Val Loss: 1.4070\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.4242... Val Loss: 1.4035\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3980... Val Loss: 1.4049\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3593... Val Loss: 1.4027\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3840... Val Loss: 1.3997\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.4079... Val Loss: 1.4051\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3847... Val Loss: 1.4004\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3772... Val Loss: 1.4020\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3974... Val Loss: 1.3990\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3915... Val Loss: 1.3990\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3804... Val Loss: 1.3942\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3970... Val Loss: 1.3898\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.3555... Val Loss: 1.3882\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.3324... Val Loss: 1.3905\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3927... Val Loss: 1.3896\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3937... Val Loss: 1.3841\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3865... Val Loss: 1.3865\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.4024... Val Loss: 1.3837\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3729... Val Loss: 1.3814\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3868... Val Loss: 1.3795\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3665... Val Loss: 1.3812\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.3439... Val Loss: 1.3801\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3981... Val Loss: 1.3791\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3598... Val Loss: 1.3921\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3634... Val Loss: 1.3811\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3597... Val Loss: 1.3700\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.3442... Val Loss: 1.3727\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.3468... Val Loss: 1.3726\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.3266... Val Loss: 1.3713\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.3574... Val Loss: 1.3692\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3623... Val Loss: 1.3699\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.3394... Val Loss: 1.3686\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.3569... Val Loss: 1.3634\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.3389... Val Loss: 1.3648\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.3481... Val Loss: 1.3632\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3513... Val Loss: 1.3647\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.3461... Val Loss: 1.3654\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.3589... Val Loss: 1.3642\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.3303... Val Loss: 1.3624\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.3270... Val Loss: 1.3583\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.3431... Val Loss: 1.3560\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.3247... Val Loss: 1.3606\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.3209... Val Loss: 1.3543\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3590... Val Loss: 1.3554\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.3324... Val Loss: 1.3561\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.3264... Val Loss: 1.3553\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.3187... Val Loss: 1.3551\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.3416... Val Loss: 1.3522\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.3149... Val Loss: 1.3493\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2820... Val Loss: 1.3529\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.3303... Val Loss: 1.3523\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.3062... Val Loss: 1.3498\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.3232... Val Loss: 1.3494\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2998... Val Loss: 1.3456\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.3179... Val Loss: 1.3448\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.3221... Val Loss: 1.3462\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.3318... Val Loss: 1.3400\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.3143... Val Loss: 1.3430\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2836... Val Loss: 1.3424\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.3006... Val Loss: 1.3418\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2999... Val Loss: 1.3411\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.3045... Val Loss: 1.3419\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.3127... Val Loss: 1.3390\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.3238... Val Loss: 1.3398\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.3258... Val Loss: 1.3396\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2984... Val Loss: 1.3362\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2974... Val Loss: 1.3374\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2990... Val Loss: 1.3328\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.3239... Val Loss: 1.3313\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.3164... Val Loss: 1.3357\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2969... Val Loss: 1.3324\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.3113... Val Loss: 1.3343\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2825... Val Loss: 1.3302\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2848... Val Loss: 1.3302\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.3057... Val Loss: 1.3299\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2937... Val Loss: 1.3290\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2903... Val Loss: 1.3272\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2824... Val Loss: 1.3265\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2753... Val Loss: 1.3286\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2968... Val Loss: 1.3270\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.3054... Val Loss: 1.3285\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.3023... Val Loss: 1.3243\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.3156... Val Loss: 1.3255\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2814... Val Loss: 1.3261\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2986... Val Loss: 1.3233\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2757... Val Loss: 1.3246\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.3090... Val Loss: 1.3212\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2630... Val Loss: 1.3202\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2735... Val Loss: 1.3222\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2806... Val Loss: 1.3217\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2648... Val Loss: 1.3236\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2736... Val Loss: 1.3189\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2830... Val Loss: 1.3209\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2889... Val Loss: 1.3180\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2827... Val Loss: 1.3171\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2917... Val Loss: 1.3136\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2875... Val Loss: 1.3156\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2724... Val Loss: 1.3176\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2854... Val Loss: 1.3120\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2515... Val Loss: 1.3138\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2522... Val Loss: 1.3125\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2452... Val Loss: 1.3135\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2482... Val Loss: 1.3143\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2593... Val Loss: 1.3108\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2453... Val Loss: 1.3136\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2929... Val Loss: 1.3115\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.3064... Val Loss: 1.3119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "HEDlskxjlKab"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = net(inputs, h)\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu()\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        return net.int2char[char], h"
      ],
      "metadata": {
        "id": "O9IbLU2mlScg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    net.eval()\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "c8Hlo-6RldWb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQzKAtkHlk85",
        "outputId": "ec7ef440-fe87-4feb-f8a7-3ee171783ca5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anna, still stoding him the study of that sense of the first.\n",
            "\n",
            "\"When he's the singer to makman some simple, with a man and a minute. It's not a pity\n",
            "of the strong table,\" answered Levin. \"And\n",
            "it is those will see her and worde tomerrach was alwored, that it is true that I\n",
            "have been so merely for the peasant.\n",
            "\n",
            "\"I don't get on worre.\"\n",
            "\n",
            "\"What so means of the most of it, a dinner are to the coundry, but I were as a meanon,\" said the criminal conversation. \"I'm not a success and than\n",
            "evin of all the sides,\"\n",
            "she say to himself to her to start, the steps.\n",
            "\n",
            "\"I cannot general than\n",
            "their mother.\"\n",
            "\n",
            "\"All the stream, that you don't be to that anything to be done to them\n",
            "the people togather and sen their\n",
            "feelings of mistaken and to be in the peasants'. Whether anything. Will the say, it's not to see it, and so,\" she said.\n",
            "\n",
            "\"I have nothing been that you say it out of this\n",
            "more all these wild marriage. And I should be a might be, and I have been the\n",
            "same and to speak of it, and to be a possible of the same\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "\n",
        "loaded = RNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmGCpE4Nnv26",
        "outputId": "2ff6fe0d-5b11-4322-b686-03a9639e0174"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-c9235bbf78e9>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(f)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W6XmWmin0NQ",
        "outputId": "b6754530-11ee-478f-c109-c4f6ab5581b5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And Levin said had no second and the country.\n",
            "The world-crowd, the mistate finished of them.\n",
            "\n",
            "\"Well, are your acquaintance we have been through to marrow;\" he went on.\n",
            "\n",
            "\"And the sick meant misery the says and arrainish.\n",
            "\n",
            "As he does not than\n",
            "a man, waiting and sorry that the wearther was a cholid trouble for\n",
            "the condringing minute at her\n",
            "fasility. And\n",
            "tharks to the country, and she would be indein to be anyway, and at the same to him, and all the forest of more talking of an official\n",
            "cape of any post and man at times.\"\n",
            "\n",
            "\"Well, and there were a condession if anyone. We was so all and saying it. I've been mading her to her food. If they can't\n",
            "did the paper, and\n",
            "this is now a most\n",
            "princess,\" he said, went in\n",
            "the\n",
            "streat of the same as it was shaked over the significance, and seemed to the middle of the cape of his brother he had at the mustrace of\n",
            "a consider of his\n",
            "stangings the parance of the sound of a porter\n",
            "who had been despair, and\n",
            "had tro with horror to her, and\n",
            "had not called a little open\n",
            "figue works. The children were not for\n",
            "a man over her, he stood sudferlly and hoped in his sister in silence, and the same, who standing his stanters. She deconding his words, he stidd before and hearing the care,\n",
            "he had thought her to see her head, was\n",
            "so\n",
            "angry for\n",
            "anyone and strange his bride--that he still made her. She was\n",
            "something with the\n",
            "meaning of them, and to clever,\n",
            "was\n",
            "it and stranged. And there intense, though the minds of such their staircase was said to be, to the mastiration to always and so married this always and careful. He flushed\n",
            "both at the\n",
            "resplorming tind of the court of his way of his shade in the same positions and had been troubed to him\n",
            "five a champaration, and the secunious of them, she came, and that the same went of\n",
            "his hose that he had thinking to\n",
            "bed a lat had any of the man, and was in it to this frame of her at the marsh.\n",
            "\n",
            "\"To be drawing about to the marsh of to me,\" he\n",
            "was not\n",
            "going at it, still as\n",
            "to her hand on the coat and\n",
            "her clear to answer. Showed him \n"
          ]
        }
      ]
    }
  ]
}